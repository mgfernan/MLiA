{
 "metadata": {
  "name": "",
  "signature": "sha256:6a87c814cc2de568bd6b61d92b0f7ee16f3e581bb96265a3436e69b443058d05"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3.- Splitting datasets one feature at a time: Decision trees\n",
      "============================================================\n",
      "\n",
      "\n",
      "3.1.- Tree construction\n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.1.- Information gain (Entropy)###\n",
      "\n",
      "As defined in [Wikipedia](http://en.wikipedia.org/wiki/Entropy_(information_theory), Entropy is **a measure of uncertainty**. Entropy is larger as the source of information is also more random. The less random, the lower the Entropy.\n",
      "\n",
      "Assuming a dicrete random variable $X$ than has the following possible values: $$ {\u00a0x_1, x_2, ..., x_N }$$\n",
      "\n",
      "will have an Entropy ($H$):\n",
      "\n",
      "$$\n",
      "H = - \\sum_{i=1}^{N} p_i \\cdot \\log_2{p_i} \n",
      "$$\n",
      "\n",
      "where $p_i$ is the probability that the value $x_i$ of the random variable $X$ occurs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "# Method to compute the entropy of a dataset\n",
      "def entropy(data_set):\n",
      "    \"\"\"\n",
      "    We are assuming that data_set is a collection of samples,\n",
      "    and each samples has some features and a label in the last field\n",
      "    \n",
      "    >>> entropy( [[1],[0]] )   # Fair coinflip, maximum uncertainty\n",
      "    1.0\n",
      "    \n",
      "    >>> entropy( [[0],[0]] )   # Loaded coinflip, previsible outcome\n",
      "    0.0\n",
      "    \"\"\"\n",
      "    \n",
      "    n_entries = len(data_set)\n",
      "    \n",
      "    # Let's count the labels so that we can then measure its \n",
      "    # probability within the data_set\n",
      "    label_counts = {}\n",
      "    \n",
      "    for sample in data_set:\n",
      "        \n",
      "        current_label = sample[-1]\n",
      "        if current_label not in label_counts:\n",
      "            label_counts[current_label] = 0\n",
      "            \n",
      "        label_counts[current_label] += 1\n",
      "        \n",
      "    # Compute the Entropy (H)\n",
      "    H = 0\n",
      "    \n",
      "    for key, count in label_counts.iteritems():\n",
      "        p = float(count) / n_entries\n",
      "        \n",
      "        H -= p * math.log(p, 2)\n",
      "    \n",
      "    return(H)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_set = [[1,1,'yes'],[1,1,'yes'], [1,0,'no'], [0,1,'no'], [0,1,'no'] ]\n",
      "entropy(data_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "0.9709505944546686"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add a little bit of uncertainty in the data set\n",
      "data_set[0][-1] = 'maybe'\n",
      "entropy(data_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "1.3709505944546687"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Gini impurity** is the probability of choosing an item from the set and the probability of that item from being missclassified"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.2.- Splitting the data set###\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split(data_set, column, value):\n",
      "    \n",
      "    split_data_set = []\n",
      "    \n",
      "    for sample in data_set:\n",
      "        if sample[column] == value:\n",
      "            \n",
      "            reduced_sample = sample[:column]\n",
      "            reduced_sample.extend( sample[column+1:] )\n",
      "            \n",
      "            split_data_set.append(reduced_sample)\n",
      "            \n",
      "    return split_data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Have a look at the data set\n",
      "data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all the features whose first column (0) takes the value of 1\n",
      "split(data_set, 0, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all the features whose first column (0) takes the other possible\n",
      "# value (0)\n",
      "split(data_set, 0, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "[[1, 'no'], [1, 'no']]"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Split based in Information Gain**\n",
      "\n",
      "The choice fo the best feature to split the data set is based on the information gain for the resulting split.\n",
      "\n",
      "Formally, the information gain is defined as\n",
      "\n",
      "> Information gain ($IG(S,A)$) due to a split of the data set \n",
      "> $S$ using feature $A$ is the reduction of entropy (disorder)\n",
      "> that results by splitting using feature $A$\n",
      "\n",
      "The expression is the following:\n",
      "\n",
      "$$\n",
      "IG(S,A) = H(S) - \\sum_{a \\, \\in \\, values(A)} \\frac{|S_a|}{|S|} \\cdot H(S_a)\n",
      "$$\n",
      "\n",
      "where $v$ denotes the value that the feature $A$ can take and $|X|$ denotes the number of elements in the set $X$. The intuition is that we to maximize the information gain when we split, meaning that we want to construct sets with the less amount of randomness possible (i.e. homogeneous split sets). In the event that the split by a feature gives random sets of elements, the part of the summatory will be close to the total entropy and therefore the information gain will be very low. In the event that there is some feature that clearly splits the data set in a group of distinct features (reducing the randomness), the second term of the information gain equation will tend to 0 (low disorder) and therefore the total information gain will be close to the total entropy, thus yielding a large information gain."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division   # To avoid rounding of integer division\n",
      "\n",
      "def choose_best_split(data_set):\n",
      "    \n",
      "    # Making the split will imply that we are going to reduce \n",
      "    # the number of features for the following splits\n",
      "    n_features = len(data_set[0]) - 1 \n",
      "    \n",
      "    # Some initializations\n",
      "    base_entropy = entropy( data_set )\n",
      "    best_info_gain = 0.0\n",
      "    best_feature = None\n",
      "    \n",
      "    # Loop over all the number of features\n",
      "    for i in range(n_features):\n",
      "        \n",
      "        # Build a list of unique values that the feature can take\n",
      "        feature_values = set( [ sample[i] for sample in data_set ] )\n",
      "        \n",
      "        # Calculate the entropy due to this split\n",
      "        new_entropy = 0.0\n",
      "        for value in feature_values:\n",
      "            sub_data_set = split(data_set, i, value)\n",
      "            \n",
      "            p = len(sub_data_set) / len(data_set)\n",
      "            new_entropy += p * entropy(sub_data_set)\n",
      "            \n",
      "        info_gain = base_entropy - new_entropy\n",
      "            \n",
      "        if info_gain > best_info_gain:\n",
      "            best_info_gain = info_gain\n",
      "            best_feature = i\n",
      "            \n",
      "    return best_feature\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "choose_best_split(data_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}